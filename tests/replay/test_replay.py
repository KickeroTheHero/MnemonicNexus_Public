#!/usr/bin/env python3
"""
Golden Replay Harness - PR-5 Implementation

Validates golden tests for hash stability, tenancy compliance, and schema validation.
Implements S0 acceptance criteria for deterministic replay.
"""

import asyncio
import json
import os
import sys
import uuid
from pathlib import Path
from typing import Any

import pytest
import yaml

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
from mnx.inference.moe_controller.controller import MoEController  # noqa: E402
from mnx.inference.moe_controller.validators import decision_hash  # noqa: E402


class ReplayHarness:
    """
    Golden test replay harness with S0 compliance validation

    Features:
    - Hash stability verification with fixed seeds
    - Tenancy field validation in all decision records
    - Schema compliance for brief.v1 outputs
    - Citation enforcement for web search usage
    - Baseline evidence integration
    """

    def __init__(self):
        self.golden_dir = project_root / "tests" / "golden"
        self.controller = None
        self.baseline_evidence = {}

    async def initialize(self):
        """Initialize controller for replay testing"""
        if not self.controller:
            self.controller = MoEController()

    async def load_golden_tests(self) -> list[dict[str, Any]]:
        """Load all golden test YAML files"""
        golden_tests = []

        for yaml_file in self.golden_dir.glob("g*.yaml"):
            if yaml_file.name.startswith("_"):
                continue  # Skip template files

            try:
                with open(yaml_file, encoding="utf-8") as f:
                    test_data = yaml.safe_load(f)
                    test_data["source_file"] = yaml_file.name
                    golden_tests.append(test_data)
            except Exception as e:
                pytest.fail(f"Failed to load golden test {yaml_file}: {e}")

        return golden_tests

    async def execute_golden_test(self, test_data: dict[str, Any]) -> dict[str, Any]:
        """Execute a single golden test with deterministic settings"""

        # Extract test parameters
        test_name = test_data.get("name", "unknown")
        prompt = test_data.get("prompt", "")
        sources = test_data.get("sources", [])
        expect = test_data.get("expect", {})
        seed = expect.get("seed", 12345)

        # Standard test tenancy context
        world_id = "550e8400-e29b-41d4-a716-446655440000"  # Fixed test UUID
        branch = "test"
        correlation_id = str(
            uuid.UUID("550e8400-e29b-41d4-a716-446655440001")
        )  # Fixed correlation

        # Override RNG seed for determinism
        os.environ["rng_seed"] = str(seed)

        try:
            # Execute decision through controller
            decision_record = await self.controller.make_decision(
                query=prompt,
                world_id=world_id,
                branch=branch,
                correlation_id=correlation_id,
            )

            return {
                "test_name": test_name,
                "decision_record": decision_record,
                "prompt": prompt,
                "sources": sources,
                "expect": expect,
                "success": True,
            }

        except Exception as e:
            return {
                "test_name": test_name,
                "decision_record": None,
                "prompt": prompt,
                "sources": sources,
                "expect": expect,
                "success": False,
                "error": str(e),
            }

    def validate_tenancy_fields(
        self, decision_record: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate S0 tenancy requirements in decision record"""

        validation_results = {
            "tenancy_compliant": True,
            "missing_fields": [],
            "invalid_fields": {},
        }

        required_fields = {
            "world_id": str,
            "branch": str,
            "correlation_id": str,
            "rank_version": (str, type(None)),  # Nullable
            "validation_failed": bool,
        }

        for field, expected_type in required_fields.items():
            if field not in decision_record:
                validation_results["missing_fields"].append(field)
                validation_results["tenancy_compliant"] = False
            else:
                value = decision_record[field]
                if not isinstance(value, expected_type):
                    validation_results["invalid_fields"][field] = {
                        "expected": str(expected_type),
                        "actual": type(value).__name__,
                        "value": value,
                    }
                    validation_results["tenancy_compliant"] = False

        # Validate correlation_id is UUID format
        if "correlation_id" in decision_record:
            try:
                uuid.UUID(decision_record["correlation_id"])
            except ValueError:
                validation_results["invalid_fields"]["correlation_id"] = {
                    "expected": "UUID format",
                    "actual": "invalid UUID",
                    "value": decision_record["correlation_id"],
                }
                validation_results["tenancy_compliant"] = False

        return validation_results

    def validate_fusion_requirements(
        self, decision_record: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate fusion/ranking requirements if FUSION_ENABLE=true"""

        fusion_enabled = os.getenv("FUSION_ENABLE", "false").lower() == "true"

        validation_results = {
            "fusion_compliant": True,
            "rank_version_present": False,
            "rank_version_in_hash": False,
        }

        if fusion_enabled:
            rank_version = decision_record.get("rank_version")

            if rank_version is None:
                validation_results["fusion_compliant"] = False
            else:
                validation_results["rank_version_present"] = True

                # Check if rank_version is included in hash computation
                # This would require re-computing the hash to verify
                validation_results["rank_version_in_hash"] = (
                    True  # Assume correct for now
                )

        return validation_results

    def validate_hash_stability(
        self, decision_record: dict[str, Any], test_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate decision hash is stable with fixed seed"""

        decision_hash_value = decision_record.get("hash", "")

        # Re-compute hash to verify determinism
        artifacts = decision_record.get("artifacts", [])
        tool_intent = {}
        brief = {}

        for artifact in artifacts:
            if artifact.get("type") == "tool_intent":
                tool_intent = artifact.get("data", {})
            elif artifact.get("type") == "brief":
                brief = artifact.get("data", {})

        recomputed_hash = decision_hash(
            controller_version=decision_record.get("controller_version", ""),
            policy_version=decision_record.get("policy_version", ""),
            prompt_version=decision_record.get("prompt_version", ""),
            rng_seed=decision_record.get("rng_seed", 12345),
            rank_version=decision_record.get("rank_version"),
            tool_intent=tool_intent,
            brief=brief,
        )

        return {
            "hash_stable": decision_hash_value == recomputed_hash,
            "original_hash": decision_hash_value,
            "recomputed_hash": recomputed_hash,
            "seed_used": decision_record.get("rng_seed", 12345),
        }

    def validate_citations_requirement(
        self, decision_record: dict[str, Any], test_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate citations are present when web search is used"""

        expect = test_data.get("expect", {})
        citations_required = expect.get("citations_required", False)

        validation_results = {
            "citations_compliant": True,
            "citations_found": [],
            "web_search_used": False,
        }

        # Check if web search was used in tool execution
        artifacts = decision_record.get("artifacts", [])
        tool_results = []

        for artifact in artifacts:
            if artifact.get("type") == "tool_results":
                tool_results = artifact.get("data", [])
                break

        # Look for web search in tool results
        for result in tool_results:
            if isinstance(result, dict) and result.get("tool") == "web_search":
                validation_results["web_search_used"] = True
                citations = result.get("citations", [])
                validation_results["citations_found"].extend(citations)

        # Validate citations if required
        if citations_required and validation_results["web_search_used"]:
            if not validation_results["citations_found"]:
                validation_results["citations_compliant"] = False

        return validation_results

    def validate_must_contain(
        self, decision_record: dict[str, Any], test_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Validate brief contains required substrings"""

        expect = test_data.get("expect", {})
        must_contain = expect.get("must_contain", [])

        validation_results = {
            "must_contain_compliant": True,
            "missing_phrases": [],
            "found_phrases": [],
        }

        # Extract brief content
        brief_content = ""
        artifacts = decision_record.get("artifacts", [])

        for artifact in artifacts:
            if artifact.get("type") == "brief":
                brief_data = artifact.get("data", {})
                brief_content = json.dumps(brief_data, indent=2)
                break

        # Check for required phrases
        for phrase in must_contain:
            if phrase in brief_content:
                validation_results["found_phrases"].append(phrase)
            else:
                validation_results["missing_phrases"].append(phrase)
                validation_results["must_contain_compliant"] = False

        return validation_results

    def generate_staleness_report(self) -> str:
        """Generate staleness badge for materialized view reads"""

        # Placeholder for staleness detection
        # In production, this would check MV refresh timestamps
        staleness_report = """
# Materialized View Staleness Report

## Test Execution: Golden Replay
- Timestamp: {timestamp}
- Environment: Test

## Materialized Views Status:
- MV lens_rel.events_mv: fresh (lag 0s)
- MV lens_graph.edges: fresh (lag 0s)
- MV lens_sem.embeddings: fresh (lag 0s)

## Notes:
- All MVs are fresh during test execution
- Production deployment should monitor staleness actively
""".format(
            timestamp=f"{uuid.uuid4()}"
        )

        return staleness_report.strip()


@pytest.mark.asyncio
class TestGoldenReplay:
    """Golden test replay suite with S0 compliance validation"""

    @pytest.fixture(autouse=True)
    async def setup_harness(self):
        """Setup replay harness for each test"""
        self.harness = ReplayHarness()
        await self.harness.initialize()
        yield

    async def test_load_golden_tests(self):
        """Test that all golden YAML files can be loaded"""
        golden_tests = await self.harness.load_golden_tests()

        # Should have g01-g06 minimum, g07 is optional
        assert (
            len(golden_tests) >= 6
        ), f"Expected at least 6 golden tests, found {len(golden_tests)}"

        # Check expected test names
        test_names = [test.get("name") for test in golden_tests]
        expected_names = [
            "policy_compare",
            "outreach_plan",
            "stakeholder_map",
            "risk_register",
            "adr_single_moe",
            "brief_synthesis",
        ]

        for expected_name in expected_names:
            assert (
                expected_name in test_names
            ), f"Missing required golden test: {expected_name}"

        print(f"âœ… Loaded {len(golden_tests)} golden tests: {test_names}")

    async def test_s0_compliance_validation(self):
        """Test S0 compliance for all golden tests"""
        golden_tests = await self.harness.load_golden_tests()

        compliance_results = []
        hash_stability_results = []

        for test_data in golden_tests:
            test_name = test_data.get("name", "unknown")
            print(f"\nğŸ§ª Testing {test_name}...")

            # Execute the golden test
            execution_result = await self.harness.execute_golden_test(test_data)

            if not execution_result["success"]:
                pytest.fail(
                    f"Golden test {test_name} execution failed: {execution_result.get('error')}"
                )

            decision_record = execution_result["decision_record"]

            # Validate tenancy fields
            tenancy_validation = self.harness.validate_tenancy_fields(decision_record)
            print(
                f"   Tenancy: {'âœ…' if tenancy_validation['tenancy_compliant'] else 'âŒ'}"
            )

            # Validate fusion requirements
            fusion_validation = self.harness.validate_fusion_requirements(
                decision_record
            )
            print(
                f"   Fusion: {'âœ…' if fusion_validation['fusion_compliant'] else 'âŒ'}"
            )

            # Validate hash stability
            hash_validation = self.harness.validate_hash_stability(
                decision_record, test_data
            )
            print(f"   Hash Stable: {'âœ…' if hash_validation['hash_stable'] else 'âŒ'}")

            # Validate citations if required
            citation_validation = self.harness.validate_citations_requirement(
                decision_record, test_data
            )
            print(
                f"   Citations: {'âœ…' if citation_validation['citations_compliant'] else 'âŒ'}"
            )

            # Validate must_contain requirements
            content_validation = self.harness.validate_must_contain(
                decision_record, test_data
            )
            print(
                f"   Content: {'âœ…' if content_validation['must_contain_compliant'] else 'âŒ'}"
            )

            # Collect results
            compliance_results.append(
                {
                    "test_name": test_name,
                    "tenancy": tenancy_validation,
                    "fusion": fusion_validation,
                    "citations": citation_validation,
                    "content": content_validation,
                }
            )

            hash_stability_results.append(
                {"test_name": test_name, "hash_validation": hash_validation}
            )

        # Assert all compliance checks passed
        failed_tests = []
        for result in compliance_results:
            test_name = result["test_name"]

            if not result["tenancy"]["tenancy_compliant"]:
                failed_tests.append(f"{test_name}: tenancy validation failed")

            if not result["fusion"]["fusion_compliant"]:
                failed_tests.append(f"{test_name}: fusion validation failed")

            if not result["citations"]["citations_compliant"]:
                failed_tests.append(f"{test_name}: citations validation failed")

            if not result["content"]["must_contain_compliant"]:
                failed_tests.append(f"{test_name}: content validation failed")

        # Assert all hashes are stable
        unstable_hashes = []
        for result in hash_stability_results:
            if not result["hash_validation"]["hash_stable"]:
                unstable_hashes.append(result["test_name"])

        if failed_tests:
            pytest.fail(
                "S0 compliance failures:\n"
                + "\n".join(f"  - {failure}" for failure in failed_tests)
            )

        if unstable_hashes:
            pytest.fail(f"Hash stability failures: {unstable_hashes}")

        print(f"\nğŸ‰ All {len(golden_tests)} golden tests passed S0 compliance!")

    async def test_hash_determinism_across_runs(self):
        """Test that hashes are identical across multiple runs with same seed"""

        # Use a simple test case
        test_data = {
            "name": "determinism_test",
            "prompt": "Simple test for hash determinism",
            "expect": {"seed": 12345, "schema": "brief.v1"},
        }

        # Run the same test 3 times
        hashes = []
        for run in range(3):
            print(f"ğŸ”„ Determinism run {run + 1}/3...")

            execution_result = await self.harness.execute_golden_test(test_data)

            if not execution_result["success"]:
                pytest.fail(
                    f"Determinism test run {run + 1} failed: {execution_result.get('error')}"
                )

            decision_record = execution_result["decision_record"]
            decision_hash_value = decision_record.get("hash", "")
            hashes.append(decision_hash_value)

        # All hashes should be identical
        unique_hashes = set(hashes)

        if len(unique_hashes) != 1:
            pytest.fail(
                f"Hash determinism failed! Got {len(unique_hashes)} unique hashes: {unique_hashes}"
            )

        print(f"âœ… Hash determinism verified: {hashes[0]}")

    async def test_schema_compliance(self):
        """Test that all briefs comply with brief.v1 schema"""
        golden_tests = await self.harness.load_golden_tests()

        schema_failures = []

        for test_data in golden_tests:
            test_name = test_data.get("name", "unknown")
            expected_schema = test_data.get("expect", {}).get("schema", "brief.v1")

            if expected_schema != "brief.v1":
                continue  # Skip non-brief tests

            execution_result = await self.harness.execute_golden_test(test_data)

            if not execution_result["success"]:
                schema_failures.append(f"{test_name}: execution failed")
                continue

            decision_record = execution_result["decision_record"]

            # Extract brief from artifacts
            brief_data = None
            artifacts = decision_record.get("artifacts", [])

            for artifact in artifacts:
                if artifact.get("type") == "brief":
                    brief_data = artifact.get("data", {})
                    break

            if not brief_data:
                schema_failures.append(f"{test_name}: no brief found in artifacts")
                continue

            # Validate against brief.v1 schema
            try:
                validator = self.harness.controller.validator
                validator.validate(brief_data, "brief.v1")
                print(f"âœ… {test_name}: brief.v1 schema compliant")
            except Exception as e:
                schema_failures.append(f"{test_name}: schema validation failed - {e}")

        if schema_failures:
            pytest.fail(
                "Schema compliance failures:\n"
                + "\n".join(f"  - {failure}" for failure in schema_failures)
            )

    def test_baseline_integration(self):
        """Test integration with baseline evidence system"""

        # Generate staleness report
        staleness_report = self.harness.generate_staleness_report()

        assert "Materialized View Staleness Report" in staleness_report
        assert "lens_rel" in staleness_report
        assert "fresh" in staleness_report or "stale" in staleness_report

        print("âœ… Baseline evidence integration functional")
        print(f"ğŸ“Š Staleness report preview:\n{staleness_report[:200]}...")

    async def test_end_to_end_golden_suite(self):
        """Comprehensive end-to-end test of entire golden suite"""

        print("\nğŸ¯ GOLDEN REPLAY SUITE - S0 ACCEPTANCE TEST")
        print("=" * 60)

        # Load all golden tests
        golden_tests = await self.harness.load_golden_tests()
        print(f"ğŸ“‹ Loaded {len(golden_tests)} golden tests")

        # Execute and validate each test
        all_results = []
        for i, test_data in enumerate(golden_tests, 1):
            test_name = test_data.get("name", "unknown")
            print(f"\n[{i}/{len(golden_tests)}] Executing {test_name}...")

            execution_result = await self.harness.execute_golden_test(test_data)
            all_results.append(execution_result)

            if execution_result["success"]:
                print("   âœ… Execution successful")

                decision_record = execution_result["decision_record"]

                # Quick validations
                tenancy_ok = self.harness.validate_tenancy_fields(decision_record)[
                    "tenancy_compliant"
                ]
                hash_ok = self.harness.validate_hash_stability(
                    decision_record, test_data
                )["hash_stable"]

                print(f"   {'âœ…' if tenancy_ok else 'âŒ'} Tenancy compliance")
                print(f"   {'âœ…' if hash_ok else 'âŒ'} Hash stability")
            else:
                print(f"   âŒ Execution failed: {execution_result.get('error')}")

        # Summary
        successful_tests = [r for r in all_results if r["success"]]
        print("\nğŸ“Š GOLDEN SUITE SUMMARY:")
        print(f"   Total Tests: {len(golden_tests)}")
        print(f"   Successful: {len(successful_tests)}")
        print(f"   Failed: {len(golden_tests) - len(successful_tests)}")

        # Generate staleness report
        staleness_report = self.harness.generate_staleness_report()
        print("\nğŸ“‹ Baseline Evidence:")
        print(staleness_report)

        # Final assertion
        if len(successful_tests) != len(golden_tests):
            failed_names = [r["test_name"] for r in all_results if not r["success"]]
            pytest.fail(f"Golden suite failures: {failed_names}")

        print("\nğŸ‰ GOLDEN REPLAY SUITE: ALL TESTS PASSED! âœ…")


# Individual test functions for pytest discovery
@pytest.mark.asyncio
async def test_golden_policy_compare():
    """Test g01_policy_compare golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g01_policy_compare.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"Policy compare test failed: {result.get('error')}"

    decision_record = result["decision_record"]
    tenancy_validation = harness.validate_tenancy_fields(decision_record)

    assert tenancy_validation[
        "tenancy_compliant"
    ], f"Tenancy validation failed: {tenancy_validation}"


@pytest.mark.asyncio
async def test_golden_outreach_plan():
    """Test g02_outreach_plan golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g02_outreach_plan.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"Outreach plan test failed: {result.get('error')}"


@pytest.mark.asyncio
async def test_golden_stakeholder_map():
    """Test g03_stakeholder_map golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g03_stakeholder_map.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"Stakeholder map test failed: {result.get('error')}"


@pytest.mark.asyncio
async def test_golden_risk_register():
    """Test g04_risk_register golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g04_risk_register.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"Risk register test failed: {result.get('error')}"


@pytest.mark.asyncio
async def test_golden_adr_single_moe():
    """Test g05_adr_single_moe golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g05_adr_single_moe.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"ADR Single-MoE test failed: {result.get('error')}"


@pytest.mark.asyncio
async def test_golden_brief_synthesis():
    """Test g06_brief_synthesis golden test"""
    harness = ReplayHarness()
    await harness.initialize()

    with open(harness.golden_dir / "g06_brief_synthesis.yaml") as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    assert result["success"], f"Brief synthesis test failed: {result.get('error')}"


@pytest.mark.asyncio
async def test_golden_peer_echo_optional():
    """Test g07_peer_echo golden test (optional)"""
    harness = ReplayHarness()
    await harness.initialize()

    peer_echo_file = harness.golden_dir / "g07_peer_echo.yaml"
    if not peer_echo_file.exists():
        pytest.skip("g07_peer_echo.yaml not present - optional test")

    with open(peer_echo_file) as f:
        test_data = yaml.safe_load(f)

    result = await harness.execute_golden_test(test_data)

    # Peer echo may fail if RAG_ENABLE=0, which is expected
    if not result["success"] and "peer" in result.get("error", "").lower():
        pytest.skip(
            f"Peer echo test skipped (expected with RAG_ENABLE=0): {result.get('error')}"
        )

    assert result["success"], f"Peer echo test failed: {result.get('error')}"


if __name__ == "__main__":
    # Direct execution for testing
    async def main():
        harness = ReplayHarness()
        await harness.initialize()

        print("ğŸ§ª Golden Replay Harness - Direct Test")

        # Load and test all golden tests
        golden_tests = await harness.load_golden_tests()
        print(f"ğŸ“‹ Found {len(golden_tests)} golden tests")

        for test_data in golden_tests:
            test_name = test_data.get("name")
            print(f"\nğŸ¯ Testing {test_name}...")

            result = await harness.execute_golden_test(test_data)
            if result["success"]:
                print(f"   âœ… {test_name} executed successfully")
            else:
                print(f"   âŒ {test_name} failed: {result.get('error')}")

    asyncio.run(main())
